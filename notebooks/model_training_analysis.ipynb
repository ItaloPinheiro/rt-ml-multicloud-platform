{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training & MLflow Integration\n",
    "\n",
    "Train fraud detection models and register them in MLflow for the ML Pipeline Platform.\n",
    "\n",
    "**Prerequisites**: Run `data_exploration.ipynb` first or have training data ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Environment ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data or generate sample\n",
    "data_path = Path('../sample_data/demo/datasets/fraud_detection_processed.csv')\n",
    "\n",
    "if data_path.exists():\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Loaded {len(df)} records from {data_path}\")\n",
    "else:\n",
    "    # Generate sample data if not exists\n",
    "    print(\"Generating sample training data...\")\n",
    "    np.random.seed(42)\n",
    "    n_samples = 5000\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'amount': np.random.lognormal(4, 2, n_samples),\n",
    "        'merchant_category': np.random.choice(['electronics', 'grocery', 'gas', 'restaurant', 'online'], n_samples),\n",
    "        'hour_of_day': np.random.randint(0, 24, n_samples),\n",
    "        'is_weekend': np.random.choice([0, 1], n_samples),\n",
    "        'risk_score': np.random.beta(2, 5, n_samples),\n",
    "        'days_since_last': np.random.exponential(5, n_samples),\n",
    "        'num_transactions_today': np.random.poisson(3, n_samples),\n",
    "        'label': np.random.choice([0, 1], n_samples, p=[0.95, 0.05])\n",
    "    })\n",
    "\n",
    "    # Make fraud look different\n",
    "    fraud_idx = df['label'] == 1\n",
    "    df.loc[fraud_idx, 'risk_score'] *= 2.5\n",
    "    df.loc[fraud_idx, 'amount'] *= 1.8\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Fraud rate: {df['label'].mean()*100:.1f}%\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for training\n",
    "categorical_cols = ['merchant_category']\n",
    "numerical_cols = ['amount', 'hour_of_day', 'is_weekend', 'risk_score',\n",
    "                  'days_since_last', 'num_transactions_today']\n",
    "\n",
    "# Encode categorical variables\n",
    "le = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    df[f'{col}_encoded'] = le.fit_transform(df[col])\n",
    "    numerical_cols.append(f'{col}_encoded')\n",
    "\n",
    "# Select features and target\n",
    "X = df[numerical_cols]\n",
    "y = df['label']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nFeatures: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MLflow Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure MLflow\n",
    "import os\n",
    "\n",
    "# Check if running in Docker or local\n",
    "mlflow_uri = os.getenv('MLFLOW_TRACKING_URI', 'http://localhost:5000')\n",
    "mlflow.set_tracking_uri(mlflow_uri)\n",
    "\n",
    "# Set experiment\n",
    "experiment_name = \"fraud_detection_notebook\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"MLflow tracking URI: {mlflow_uri}\")\n",
    "print(f\"Experiment: {experiment_name}\")\n",
    "\n",
    "# Get experiment info\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if experiment:\n",
    "    print(f\"Experiment ID: {experiment.experiment_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_log_model(name, model, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Train model and log to MLflow\"\"\"\n",
    "\n",
    "    with mlflow.start_run(run_name=name):\n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'auc_roc': roc_auc_score(y_test, y_pred_proba)\n",
    "        }\n",
    "\n",
    "        # Log parameters\n",
    "        if hasattr(model, 'get_params'):\n",
    "            mlflow.log_params(model.get_params())\n",
    "\n",
    "        # Log metrics\n",
    "        mlflow.log_metrics(metrics)\n",
    "\n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(\n",
    "            sk_model=model,\n",
    "            artifact_path=\"model\",\n",
    "            registered_model_name=\"fraud_detector\"\n",
    "        )\n",
    "\n",
    "        # Log feature importance if available\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importance = pd.DataFrame({\n",
    "                'feature': X_train.columns,\n",
    "                'importance': model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "\n",
    "            # Log as artifact\n",
    "            importance.to_csv('/tmp/feature_importance.csv', index=False)\n",
    "            mlflow.log_artifact('/tmp/feature_importance.csv')\n",
    "\n",
    "        print(f\"\\n{name} Results:\")\n",
    "        print(\"-\" * 40)\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric:10s}: {value:.4f}\")\n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train different model configurations\n",
    "models_to_train = [\n",
    "    (\"RF_baseline\", RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42)),\n",
    "    (\"RF_medium\", RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)),\n",
    "    (\"RF_advanced\", RandomForestClassifier(n_estimators=200, max_depth=15, min_samples_split=5, random_state=42)),\n",
    "]\n",
    "\n",
    "results = {}\n",
    "for name, model in models_to_train:\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    metrics = train_and_log_model(name, model, X_train, X_test, y_train, y_test)\n",
    "    results[name] = metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame(results).T\n",
    "comparison_df = comparison_df.round(4)\n",
    "\n",
    "print(\"\\nMODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df)\n",
    "\n",
    "# Find best model by F1 score\n",
    "best_model = comparison_df['f1'].idxmax()\n",
    "print(f\"\\nüèÜ Best model (by F1 score): {best_model}\")\n",
    "print(f\"   F1 Score: {comparison_df.loc[best_model, 'f1']:.4f}\")\n",
    "print(f\"   AUC-ROC: {comparison_df.loc[best_model, 'auc_roc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Metrics comparison\n",
    "comparison_df[['accuracy', 'precision', 'recall', 'f1']].plot(kind='bar', ax=axes[0])\n",
    "axes[0].set_title('Model Performance Metrics')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].legend(loc='best')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# AUC-ROC comparison\n",
    "comparison_df['auc_roc'].plot(kind='bar', ax=axes[1], color='orange')\n",
    "axes[1].set_title('AUC-ROC Scores')\n",
    "axes[1].set_ylabel('AUC-ROC')\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Promote Best Model to Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get MLflow client\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# Get all versions of the model\n",
    "model_name = \"fraud_detector\"\n",
    "\n",
    "try:\n",
    "    versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "    print(f\"Found {len(versions)} versions of {model_name}\\n\")\n",
    "\n",
    "    # Display version info\n",
    "    for v in versions[:5]:  # Show last 5 versions\n",
    "        run = client.get_run(v.run_id)\n",
    "        metrics = run.data.metrics\n",
    "        print(f\"Version {v.version}:\")\n",
    "        print(f\"  Run Name: {run.info.run_name}\")\n",
    "        print(f\"  F1 Score: {metrics.get('f1', 'N/A'):.4f}\" if 'f1' in metrics else \"  F1 Score: N/A\")\n",
    "        print(f\"  Stage: {v.current_stage}\")\n",
    "        print()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "    print(\"Model will be registered when you run the training cells above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition best model to production\n",
    "try:\n",
    "    # Get latest version\n",
    "    latest_version = versions[0].version if versions else None\n",
    "\n",
    "    if latest_version:\n",
    "        # Transition to production\n",
    "        client.transition_model_version_stage(\n",
    "            name=model_name,\n",
    "            version=latest_version,\n",
    "            stage=\"Production\",\n",
    "            archive_existing_versions=True\n",
    "        )\n",
    "\n",
    "        print(f\"‚úÖ Model version {latest_version} promoted to Production!\")\n",
    "        print(\"\\nThe API will automatically load this model within 60 seconds.\")\n",
    "        print(f\"\\nYou can verify at: {mlflow_uri}/#/models/{model_name}\")\n",
    "    else:\n",
    "        print(\"No model versions found. Train a model first.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not promote model: {e}\")\n",
    "    print(\"Make sure MLflow server is running and accessible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load production model for testing\n",
    "try:\n",
    "    model_uri = f\"models:/{model_name}/Production\"\n",
    "    loaded_model = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "    # Test prediction\n",
    "    sample = X_test.iloc[:5]\n",
    "    predictions = loaded_model.predict(sample)\n",
    "    probabilities = loaded_model.predict_proba(sample)[:, 1]\n",
    "\n",
    "    print(\"Production Model Test:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Model: {model_uri}\")\n",
    "    print(\"\\nSample predictions:\")\n",
    "    for i, (pred, prob) in enumerate(zip(predictions, probabilities)):\n",
    "        actual = y_test.iloc[i]\n",
    "        print(f\"  Sample {i+1}: Predicted={pred}, Probability={prob:.3f}, Actual={actual}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not load production model: {e}\")\n",
    "    print(\"Make sure a model is in Production stage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate API Request Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate example API requests\n",
    "import json\n",
    "\n",
    "# Create example requests\n",
    "normal_transaction = {\n",
    "    \"features\": {\n",
    "        \"amount\": 50.0,\n",
    "        \"merchant_category\": \"grocery\",\n",
    "        \"hour_of_day\": 14,\n",
    "        \"is_weekend\": 0,\n",
    "        \"risk_score\": 0.2,\n",
    "        \"days_since_last\": 2,\n",
    "        \"num_transactions_today\": 3\n",
    "    },\n",
    "    \"model_name\": \"fraud_detector\",\n",
    "    \"return_probabilities\": True\n",
    "}\n",
    "\n",
    "suspicious_transaction = {\n",
    "    \"features\": {\n",
    "        \"amount\": 2500.0,\n",
    "        \"merchant_category\": \"electronics\",\n",
    "        \"hour_of_day\": 3,\n",
    "        \"is_weekend\": 1,\n",
    "        \"risk_score\": 0.8,\n",
    "        \"days_since_last\": 30,\n",
    "        \"num_transactions_today\": 15\n",
    "    },\n",
    "    \"model_name\": \"fraud_detector\",\n",
    "    \"return_probabilities\": True\n",
    "}\n",
    "\n",
    "print(\"Example API Requests:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n1. Normal Transaction:\")\n",
    "print(json.dumps(normal_transaction, indent=2))\n",
    "print(\"\\n2. Suspicious Transaction:\")\n",
    "print(json.dumps(suspicious_transaction, indent=2))\n",
    "\n",
    "# Save to files\n",
    "output_dir = Path('../sample_data/demo/requests')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_dir / 'normal_transaction.json', 'w') as f:\n",
    "    json.dump(normal_transaction, f, indent=2)\n",
    "\n",
    "with open(output_dir / 'suspicious_transaction.json', 'w') as f:\n",
    "    json.dump(suspicious_transaction, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Saved example requests to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test with API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test API endpoint (if running)\n",
    "import requests\n",
    "\n",
    "api_url = \"http://localhost:8000\"\n",
    "\n",
    "try:\n",
    "    # Check API health\n",
    "    response = requests.get(f\"{api_url}/health\")\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(\"‚úÖ API is running!\\n\")\n",
    "\n",
    "        # Test prediction\n",
    "        response = requests.post(\n",
    "            f\"{api_url}/predict\",\n",
    "            json=normal_transaction\n",
    "        )\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            print(\"API Prediction Result:\")\n",
    "            print(json.dumps(result, indent=2))\n",
    "        else:\n",
    "            print(f\"Prediction failed: {response.status_code}\")\n",
    "            print(response.text)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è API is not responding. Make sure Docker services are running.\")\n",
    "        print(\"Run: docker-compose up -d\")\n",
    "\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"‚ö†Ô∏è Cannot connect to API at http://localhost:8000\")\n",
    "    print(\"\\nTo start the API:\")\n",
    "    print(\"1. Run: docker-compose up -d\")\n",
    "    print(\"2. Wait 30 seconds for services to start\")\n",
    "    print(\"3. Try again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Data Preparation**: Loading and preparing features for model training\n",
    "2. **MLflow Integration**: Tracking experiments and model versions\n",
    "3. **Model Training**: Training multiple model configurations\n",
    "4. **Model Comparison**: Evaluating and comparing model performance\n",
    "5. **Model Promotion**: Transitioning best model to production\n",
    "6. **API Integration**: Testing model serving through the FastAPI endpoint\n",
    "\n",
    "**Next Steps**:\n",
    "- Monitor model performance with `performance_monitoring.ipynb`\n",
    "- Set up automated retraining pipeline\n",
    "- Configure alerts for model drift\n",
    "- Deploy to production environment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
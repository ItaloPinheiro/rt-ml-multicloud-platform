{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Monitoring - ML Pipeline Platform\n",
    "\n",
    "Monitor API performance, model metrics, and system health in real-time.\n",
    "\n",
    "**Prerequisites**: Services should be running (`docker-compose up -d`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from IPython.display import display, clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "API_URL = \"http://localhost:8000\"\n",
    "PROMETHEUS_URL = \"http://localhost:9090\"\n",
    "GRAFANA_URL = \"http://localhost:3001\"\n",
    "MLFLOW_URL = \"http://localhost:5000\"\n",
    "\n",
    "print(\"Environment ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Service Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_service_health():\n",
    "    \"\"\"Check health of all services\"\"\"\n",
    "    services = {\n",
    "        \"API\": f\"{API_URL}/health\",\n",
    "        \"Prometheus\": f\"{PROMETHEUS_URL}/-/healthy\",\n",
    "        \"MLflow\": f\"{MLFLOW_URL}/health\",\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, url in services.items():\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                results[name] = \"‚úÖ Healthy\"\n",
    "                if name == \"API\" and response.headers.get('content-type') == 'application/json':\n",
    "                    data = response.json()\n",
    "                    if 'model_loaded' in data:\n",
    "                        results[name] += f\" (Model: {data.get('model_name', 'Unknown')})\"\n",
    "            else:\n",
    "                results[name] = f\"‚ö†Ô∏è Unhealthy (Status: {response.status_code})\"\n",
    "        except requests.exceptions.RequestException:\n",
    "            results[name] = \"‚ùå Unavailable\"\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Check services\n",
    "print(\"SERVICE HEALTH STATUS\")\n",
    "print(\"=\"*50)\n",
    "health_results = check_service_health()\n",
    "for service, status in health_results.items():\n",
    "    print(f\"{service:15s}: {status}\")\n",
    "\n",
    "if \"‚ùå\" in str(health_results.values()):\n",
    "    print(\"\\n‚ö†Ô∏è Some services are unavailable. Run: docker-compose up -d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. API Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api_metrics():\n",
    "    \"\"\"Fetch current API metrics from Prometheus endpoint\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{API_URL}/metrics\")\n",
    "        if response.status_code == 200:\n",
    "            metrics_text = response.text\n",
    "            \n",
    "            # Parse key metrics\n",
    "            metrics = {}\n",
    "            for line in metrics_text.split('\\n'):\n",
    "                if line and not line.startswith('#'):\n",
    "                    if 'http_requests_total' in line:\n",
    "                        parts = line.split(' ')\n",
    "                        if len(parts) == 2:\n",
    "                            metrics['total_requests'] = float(parts[1])\n",
    "                    elif 'http_request_duration_seconds_sum' in line:\n",
    "                        parts = line.split(' ')\n",
    "                        if len(parts) == 2:\n",
    "                            metrics['total_duration'] = float(parts[1])\n",
    "                    elif 'predictions_total' in line:\n",
    "                        parts = line.split(' ')\n",
    "                        if len(parts) == 2:\n",
    "                            metrics['total_predictions'] = float(parts[1])\n",
    "                    elif 'model_cache_hits_total' in line:\n",
    "                        parts = line.split(' ')\n",
    "                        if len(parts) == 2:\n",
    "                            metrics['cache_hits'] = float(parts[1])\n",
    "                    elif 'model_cache_misses_total' in line:\n",
    "                        parts = line.split(' ')\n",
    "                        if len(parts) == 2:\n",
    "                            metrics['cache_misses'] = float(parts[1])\n",
    "            return metrics\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Get current metrics\n",
    "metrics = get_api_metrics()\n",
    "if metrics:\n",
    "    print(\"API METRICS SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total Requests: {metrics.get('total_requests', 0):.0f}\")\n",
    "    print(f\"Total Predictions: {metrics.get('total_predictions', 0):.0f}\")\n",
    "    \n",
    "    cache_hits = metrics.get('cache_hits', 0)\n",
    "    cache_misses = metrics.get('cache_misses', 0)\n",
    "    if cache_hits + cache_misses > 0:\n",
    "        cache_rate = cache_hits / (cache_hits + cache_misses) * 100\n",
    "        print(f\"Cache Hit Rate: {cache_rate:.1f}%\")\n",
    "    \n",
    "    if metrics.get('total_requests', 0) > 0:\n",
    "        avg_duration = metrics.get('total_duration', 0) / metrics.get('total_requests', 1)\n",
    "        print(f\"Avg Response Time: {avg_duration*1000:.1f}ms\")\n",
    "else:\n",
    "    print(\"No metrics available yet. Make some API calls first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test(num_requests=50, concurrent=5):\n",
    "    \"\"\"Simple load test to generate metrics\"\"\"\n",
    "    import concurrent.futures\n",
    "    \n",
    "    # Sample request\n",
    "    request_data = {\n",
    "        \"features\": {\n",
    "            \"amount\": 150.0,\n",
    "            \"merchant_category\": \"electronics\",\n",
    "            \"hour_of_day\": 14,\n",
    "            \"is_weekend\": 0,\n",
    "            \"risk_score\": 0.3,\n",
    "            \"days_since_last\": 3,\n",
    "            \"num_transactions_today\": 2\n",
    "        },\n",
    "        \"model_name\": \"fraud_detector\"\n",
    "    }\n",
    "    \n",
    "    def make_request(_):\n",
    "        start = time.time()\n",
    "        try:\n",
    "            response = requests.post(f\"{API_URL}/predict\", json=request_data, timeout=10)\n",
    "            duration = time.time() - start\n",
    "            return {\"success\": response.status_code == 200, \"duration\": duration}\n",
    "        except:\n",
    "            return {\"success\": False, \"duration\": time.time() - start}\n",
    "    \n",
    "    print(f\"Starting load test: {num_requests} requests with {concurrent} concurrent workers\")\n",
    "    print(\"Progress: \", end=\"\")\n",
    "    \n",
    "    results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent) as executor:\n",
    "        futures = [executor.submit(make_request, i) for i in range(num_requests)]\n",
    "        for i, future in enumerate(concurrent.futures.as_completed(futures)):\n",
    "            results.append(future.result())\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"{i+1}\", end=\" \")\n",
    "    \n",
    "    print(\"\\n\\nLOAD TEST RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    successes = [r for r in results if r[\"success\"]]\n",
    "    failures = [r for r in results if not r[\"success\"]]\n",
    "    durations = [r[\"duration\"] for r in successes]\n",
    "    \n",
    "    print(f\"Success Rate: {len(successes)}/{num_requests} ({len(successes)/num_requests*100:.1f}%)\")\n",
    "    \n",
    "    if durations:\n",
    "        print(f\"\\nResponse Times (successful):\")\n",
    "        print(f\"  Min: {min(durations)*1000:.1f}ms\")\n",
    "        print(f\"  Avg: {np.mean(durations)*1000:.1f}ms\")\n",
    "        print(f\"  Max: {max(durations)*1000:.1f}ms\")\n",
    "        print(f\"  P50: {np.percentile(durations, 50)*1000:.1f}ms\")\n",
    "        print(f\"  P95: {np.percentile(durations, 95)*1000:.1f}ms\")\n",
    "        print(f\"  P99: {np.percentile(durations, 99)*1000:.1f}ms\")\n",
    "    \n",
    "    if failures:\n",
    "        print(f\"\\n‚ö†Ô∏è {len(failures)} requests failed\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run load test\n",
    "try:\n",
    "    results = load_test(num_requests=50, concurrent=5)\n",
    "except Exception as e:\n",
    "    print(f\"Load test failed: {e}\")\n",
    "    print(\"Make sure the API is running and a model is loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Response Time Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize response times from load test\n",
    "if 'results' in locals() and results:\n",
    "    durations_ms = [r['duration']*1000 for r in results if r['success']]\n",
    "    \n",
    "    if durations_ms:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Histogram\n",
    "        axes[0].hist(durations_ms, bins=20, alpha=0.7, color='blue', edgecolor='black')\n",
    "        axes[0].axvline(np.mean(durations_ms), color='red', linestyle='--', label=f'Mean: {np.mean(durations_ms):.1f}ms')\n",
    "        axes[0].axvline(np.percentile(durations_ms, 95), color='orange', linestyle='--', label=f'P95: {np.percentile(durations_ms, 95):.1f}ms')\n",
    "        axes[0].set_xlabel('Response Time (ms)')\n",
    "        axes[0].set_ylabel('Frequency')\n",
    "        axes[0].set_title('Response Time Distribution')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Time series\n",
    "        axes[1].plot(durations_ms, alpha=0.7)\n",
    "        axes[1].axhline(np.mean(durations_ms), color='red', linestyle='--', alpha=0.5)\n",
    "        axes[1].set_xlabel('Request Number')\n",
    "        axes[1].set_ylabel('Response Time (ms)')\n",
    "        axes[1].set_title('Response Time Over Time')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No load test results to visualize. Run the load test cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Real-time Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_realtime(duration_seconds=30, interval=2):\n",
    "    \"\"\"Monitor API in real-time\"\"\"\n",
    "    print(f\"Monitoring for {duration_seconds} seconds...\")\n",
    "    print(\"Press Interrupt to stop early\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    metrics_history = []\n",
    "    \n",
    "    try:\n",
    "        while time.time() - start_time < duration_seconds:\n",
    "            # Get current metrics\n",
    "            metrics = get_api_metrics()\n",
    "            if metrics:\n",
    "                metrics['timestamp'] = datetime.now()\n",
    "                metrics_history.append(metrics)\n",
    "            \n",
    "            # Make a test request\n",
    "            test_request = {\n",
    "                \"features\": {\n",
    "                    \"amount\": np.random.uniform(10, 1000),\n",
    "                    \"merchant_category\": np.random.choice(['electronics', 'grocery', 'gas']),\n",
    "                    \"hour_of_day\": np.random.randint(0, 24),\n",
    "                    \"is_weekend\": np.random.choice([0, 1]),\n",
    "                    \"risk_score\": np.random.uniform(0, 1),\n",
    "                    \"days_since_last\": np.random.randint(1, 30),\n",
    "                    \"num_transactions_today\": np.random.randint(1, 10)\n",
    "                },\n",
    "                \"model_name\": \"fraud_detector\"\n",
    "            }\n",
    "            \n",
    "            request_start = time.time()\n",
    "            try:\n",
    "                response = requests.post(f\"{API_URL}/predict\", json=test_request, timeout=5)\n",
    "                request_time = (time.time() - request_start) * 1000\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    \n",
    "                    # Display current status\n",
    "                    clear_output(wait=True)\n",
    "                    print(f\"‚è±Ô∏è Time Elapsed: {time.time() - start_time:.0f}s / {duration_seconds}s\")\n",
    "                    print(f\"\\nüìä Latest Request:\")\n",
    "                    print(f\"  Response Time: {request_time:.1f}ms\")\n",
    "                    print(f\"  Prediction: {result.get('prediction', 'N/A')}\")\n",
    "                    if 'probability' in result:\n",
    "                        print(f\"  Confidence: {result['probability']:.3f}\")\n",
    "                    \n",
    "                    if len(metrics_history) > 1:\n",
    "                        print(f\"\\nüìà Session Stats:\")\n",
    "                        print(f\"  Total Requests: {len(metrics_history)}\")\n",
    "                        print(f\"  Avg Response: {np.mean([m.get('total_duration', 0) for m in metrics_history[-10:]])*1000:.1f}ms\")\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            time.sleep(interval)\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nMonitoring stopped by user\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Monitoring complete. Made {len(metrics_history)} requests.\")\n",
    "    return metrics_history\n",
    "\n",
    "# Start monitoring\n",
    "metrics_history = monitor_realtime(duration_seconds=20, interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Performance Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current model info\n",
    "try:\n",
    "    response = requests.get(f\"{API_URL}/models/current\")\n",
    "    if response.status_code == 200:\n",
    "        model_info = response.json()\n",
    "        \n",
    "        print(\"CURRENT MODEL INFO\")\n",
    "        print(\"=\"*50)\n",
    "        for key, value in model_info.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "        \n",
    "        # Get model versions from MLflow\n",
    "        import mlflow\n",
    "        from mlflow.tracking import MlflowClient\n",
    "        \n",
    "        mlflow.set_tracking_uri(MLFLOW_URL)\n",
    "        client = MlflowClient()\n",
    "        \n",
    "        try:\n",
    "            model_name = model_info.get('name', 'fraud_detector')\n",
    "            versions = client.search_model_versions(f\"name='{model_name}'\")\n",
    "            \n",
    "            if versions:\n",
    "                print(f\"\\nMODEL VERSION HISTORY\")\n",
    "                print(\"=\"*50)\n",
    "                \n",
    "                version_data = []\n",
    "                for v in versions[:5]:  # Last 5 versions\n",
    "                    run = client.get_run(v.run_id)\n",
    "                    metrics = run.data.metrics\n",
    "                    version_data.append({\n",
    "                        'Version': v.version,\n",
    "                        'Stage': v.current_stage,\n",
    "                        'F1': metrics.get('f1', 0),\n",
    "                        'AUC': metrics.get('auc_roc', 0),\n",
    "                        'Accuracy': metrics.get('accuracy', 0)\n",
    "                    })\n",
    "                \n",
    "                df_versions = pd.DataFrame(version_data)\n",
    "                print(df_versions.to_string(index=False))\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\nCould not fetch MLflow model versions: {e}\")\n",
    "            \n",
    "    else:\n",
    "        print(\"Could not get current model info. Is a model loaded?\")\n",
    "        \n",
    "except requests.exceptions.RequestException:\n",
    "    print(\"API is not responding. Make sure services are running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. System Resource Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docker_stats():\n",
    "    \"\"\"Get Docker container resource usage\"\"\"\n",
    "    import subprocess\n",
    "    \n",
    "    try:\n",
    "        # Run docker stats command\n",
    "        result = subprocess.run(\n",
    "            ['docker', 'stats', '--no-stream', '--format', \n",
    "             'table {{.Container}}\\t{{.Name}}\\t{{.CPUPerc}}\\t{{.MemUsage}}\\t{{.MemPerc}}'],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"DOCKER CONTAINER STATS\")\n",
    "            print(\"=\"*80)\n",
    "            print(result.stdout)\n",
    "        else:\n",
    "            print(\"Could not get Docker stats. Make sure Docker is running.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting Docker stats: {e}\")\n",
    "        print(\"\\nAlternative: Run 'docker stats' in a terminal\")\n",
    "\n",
    "get_docker_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Alert Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_alerts():\n",
    "    \"\"\"Check for potential issues that would trigger alerts\"\"\"\n",
    "    alerts = []\n",
    "    \n",
    "    # Check API response time\n",
    "    try:\n",
    "        start = time.time()\n",
    "        response = requests.get(f\"{API_URL}/health\", timeout=5)\n",
    "        response_time = (time.time() - start) * 1000\n",
    "        \n",
    "        if response_time > 1000:\n",
    "            alerts.append(f\"‚ö†Ô∏è HIGH LATENCY: API response time {response_time:.0f}ms > 1000ms\")\n",
    "        elif response_time > 500:\n",
    "            alerts.append(f\"‚ö†Ô∏è WARNING: API response time {response_time:.0f}ms > 500ms\")\n",
    "    except:\n",
    "        alerts.append(\"‚ùå CRITICAL: API is not responding\")\n",
    "    \n",
    "    # Check model status\n",
    "    try:\n",
    "        response = requests.get(f\"{API_URL}/models/current\")\n",
    "        if response.status_code != 200:\n",
    "            alerts.append(\"‚ö†Ô∏è WARNING: No model loaded in API\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Check error rate (simulated)\n",
    "    metrics = get_api_metrics()\n",
    "    if metrics and metrics.get('total_requests', 0) > 100:\n",
    "        # Simulate error rate check\n",
    "        error_rate = np.random.uniform(0, 0.1)  # Simulated\n",
    "        if error_rate > 0.05:\n",
    "            alerts.append(f\"‚ö†Ô∏è HIGH ERROR RATE: {error_rate*100:.1f}% > 5%\")\n",
    "    \n",
    "    print(\"ALERT STATUS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if alerts:\n",
    "        for alert in alerts:\n",
    "            print(alert)\n",
    "    else:\n",
    "        print(\"‚úÖ No alerts - All systems operating normally\")\n",
    "    \n",
    "    return alerts\n",
    "\n",
    "alerts = check_alerts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance Summary Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary dashboard\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE MONITORING DASHBOARD\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print()\n",
    "\n",
    "# Service Status\n",
    "print(\"üì° SERVICE STATUS\")\n",
    "print(\"-\"*40)\n",
    "health = check_service_health()\n",
    "for service, status in health.items():\n",
    "    print(f\"  {service:15s}: {status}\")\n",
    "\n",
    "# Performance Metrics\n",
    "print(\"\\n‚ö° PERFORMANCE METRICS\")\n",
    "print(\"-\"*40)\n",
    "metrics = get_api_metrics()\n",
    "if metrics:\n",
    "    print(f\"  Total Requests: {metrics.get('total_requests', 0):.0f}\")\n",
    "    print(f\"  Cache Hit Rate: {metrics.get('cache_hits', 0)/(metrics.get('cache_hits', 0)+metrics.get('cache_misses', 1))*100:.1f}%\")\n",
    "    if metrics.get('total_requests', 0) > 0:\n",
    "        print(f\"  Avg Response: {metrics.get('total_duration', 0)/metrics.get('total_requests', 1)*1000:.1f}ms\")\n",
    "else:\n",
    "    print(\"  No metrics available\")\n",
    "\n",
    "# Alert Summary\n",
    "print(\"\\nüö® ALERTS\")\n",
    "print(\"-\"*40)\n",
    "if alerts:\n",
    "    for alert in alerts[:3]:  # Show top 3 alerts\n",
    "        print(f\"  {alert}\")\n",
    "else:\n",
    "    print(\"  ‚úÖ No active alerts\")\n",
    "\n",
    "# Quick Links\n",
    "print(\"\\nüîó QUICK LINKS\")\n",
    "print(\"-\"*40)\n",
    "print(f\"  API Docs: {API_URL}/docs\")\n",
    "print(f\"  MLflow UI: {MLFLOW_URL}\")\n",
    "print(f\"  Grafana: {GRAFANA_URL}\")\n",
    "print(f\"  Prometheus: {PROMETHEUS_URL}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This monitoring notebook provides:\n",
    "\n",
    "1. **Health Checks**: Verify all services are running\n",
    "2. **Performance Metrics**: Track API response times and throughput\n",
    "3. **Load Testing**: Generate traffic to test system capacity\n",
    "4. **Real-time Monitoring**: Watch system behavior live\n",
    "5. **Alert Detection**: Identify potential issues\n",
    "6. **Resource Usage**: Monitor Docker container resources\n",
    "\n",
    "**Next Steps**:\n",
    "- Set up automated alerts in Grafana\n",
    "- Configure Prometheus recording rules\n",
    "- Implement SLO/SLI tracking\n",
    "- Add custom business metrics\n",
    "- Set up distributed tracing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
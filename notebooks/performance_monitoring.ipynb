{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ML Pipeline Platform - Performance Monitoring\n",
        "\n",
        "This notebook provides comprehensive performance monitoring and system analytics for the ML Pipeline Platform.\n",
        "\n",
        "## Contents\n",
        "1. [System Metrics Collection](#system-metrics)\n",
        "2. [Model Performance Tracking](#model-performance)\n",
        "3. [Data Quality Monitoring](#data-quality)\n",
        "4. [Real-time Dashboard Simulation](#dashboard)\n",
        "5. [Alert System Analysis](#alerts)\n",
        "6. [Performance Optimization](#optimization)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Time and monitoring libraries\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "import random\n",
        "import json\n",
        "\n",
        "# Statistical libraries\n",
        "from scipy import stats\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"Notebook started at: {datetime.now()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. System Metrics Collection {#system-metrics}\n",
        "\n",
        "Simulate and analyze system performance metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic system metrics data\n",
        "def generate_system_metrics(days=7, interval_minutes=5):\n",
        "    \"\"\"Generate synthetic system metrics for monitoring simulation\"\"\"\n",
        "    \n",
        "    # Calculate number of data points\n",
        "    total_minutes = days * 24 * 60\n",
        "    num_points = total_minutes // interval_minutes\n",
        "    \n",
        "    # Generate timestamps\n",
        "    start_time = datetime.now() - timedelta(days=days)\n",
        "    timestamps = [start_time + timedelta(minutes=i*interval_minutes) for i in range(num_points)]\n",
        "    \n",
        "    # Generate realistic metrics with patterns\n",
        "    metrics = []\n",
        "    \n",
        "    for i, ts in enumerate(timestamps):\n",
        "        # Add daily patterns (higher load during business hours)\n",
        "        hour = ts.hour\n",
        "        daily_factor = 1.0 + 0.5 * np.sin((hour - 6) * np.pi / 12) if 6 <= hour <= 18 else 0.3\n",
        "        \n",
        "        # Add weekly patterns (lower load on weekends)\n",
        "        weekly_factor = 0.6 if ts.weekday() >= 5 else 1.0\n",
        "        \n",
        "        # Base load with some randomness\n",
        "        base_factor = daily_factor * weekly_factor\n",
        "        \n",
        "        # System metrics\n",
        "        cpu_usage = max(0, min(100, 30 * base_factor + np.random.normal(0, 10)))\n",
        "        memory_usage = max(0, min(100, 40 * base_factor + np.random.normal(0, 8)))\n",
        "        \n",
        "        # API metrics\n",
        "        requests_per_second = max(0, 50 * base_factor + np.random.normal(0, 15))\n",
        "        response_time = max(0, 100 + 50 * base_factor + np.random.exponential(20))\n",
        "        error_rate = max(0, min(10, 0.5 + np.random.exponential(0.5)))\n",
        "        \n",
        "        # Model metrics\n",
        "        predictions_per_minute = max(0, requests_per_second * 0.8 + np.random.normal(0, 5))\n",
        "        model_accuracy = max(0.8, min(1.0, 0.95 + np.random.normal(0, 0.02)))\n",
        "        \n",
        "        # Storage metrics\n",
        "        disk_usage = min(100, 60 + i * 0.01 + np.random.normal(0, 2))  # Gradually increasing\n",
        "        \n",
        "        metrics.append({\n",
        "            'timestamp': ts,\n",
        "            'cpu_usage': cpu_usage,\n",
        "            'memory_usage': memory_usage,\n",
        "            'requests_per_second': requests_per_second,\n",
        "            'response_time_ms': response_time,\n",
        "            'error_rate': error_rate,\n",
        "            'predictions_per_minute': predictions_per_minute,\n",
        "            'model_accuracy': model_accuracy,\n",
        "            'disk_usage': disk_usage\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(metrics)\n",
        "\n",
        "# Generate metrics data\n",
        "metrics_df = generate_system_metrics(days=7, interval_minutes=5)\n",
        "print(f\"Generated {len(metrics_df)} system metric records\")\n",
        "print(f\"Time range: {metrics_df['timestamp'].min()} to {metrics_df['timestamp'].max()}\")\n",
        "print(\"\\nSample data:\")\n",
        "print(metrics_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# System metrics overview\n",
        "fig = make_subplots(\n",
        "    rows=3, cols=2,\n",
        "    subplot_titles=('CPU Usage (%)', 'Memory Usage (%)', \n",
        "                   'Response Time (ms)', 'Error Rate (%)',\n",
        "                   'Requests/Second', 'Model Accuracy'),\n",
        "    vertical_spacing=0.08\n",
        ")\n",
        "\n",
        "# CPU Usage\n",
        "fig.add_trace(go.Scatter(x=metrics_df['timestamp'], y=metrics_df['cpu_usage'],\n",
        "                        mode='lines', name='CPU Usage', line=dict(color='blue')),\n",
        "             row=1, col=1)\n",
        "\n",
        "# Memory Usage\n",
        "fig.add_trace(go.Scatter(x=metrics_df['timestamp'], y=metrics_df['memory_usage'],\n",
        "                        mode='lines', name='Memory Usage', line=dict(color='green')),\n",
        "             row=1, col=2)\n",
        "\n",
        "# Response Time\n",
        "fig.add_trace(go.Scatter(x=metrics_df['timestamp'], y=metrics_df['response_time_ms'],\n",
        "                        mode='lines', name='Response Time', line=dict(color='orange')),\n",
        "             row=2, col=1)\n",
        "\n",
        "# Error Rate\n",
        "fig.add_trace(go.Scatter(x=metrics_df['timestamp'], y=metrics_df['error_rate'],\n",
        "                        mode='lines', name='Error Rate', line=dict(color='red')),\n",
        "             row=2, col=2)\n",
        "\n",
        "# Requests per Second\n",
        "fig.add_trace(go.Scatter(x=metrics_df['timestamp'], y=metrics_df['requests_per_second'],\n",
        "                        mode='lines', name='Requests/Second', line=dict(color='purple')),\n",
        "             row=3, col=1)\n",
        "\n",
        "# Model Accuracy\n",
        "fig.add_trace(go.Scatter(x=metrics_df['timestamp'], y=metrics_df['model_accuracy'],\n",
        "                        mode='lines', name='Model Accuracy', line=dict(color='darkgreen')),\n",
        "             row=3, col=2)\n",
        "\n",
        "fig.update_layout(height=1000, title_text=\"System Performance Metrics Overview\", showlegend=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate key performance indicators (KPIs)\n",
        "current_time = metrics_df['timestamp'].max()\n",
        "last_hour_data = metrics_df[metrics_df['timestamp'] >= current_time - timedelta(hours=1)]\n",
        "last_24h_data = metrics_df[metrics_df['timestamp'] >= current_time - timedelta(days=1)]\n",
        "\n",
        "kpis = {\n",
        "    'Current Performance': {\n",
        "        'CPU Usage (%)': metrics_df['cpu_usage'].iloc[-1],\n",
        "        'Memory Usage (%)': metrics_df['memory_usage'].iloc[-1],\n",
        "        'Response Time (ms)': metrics_df['response_time_ms'].iloc[-1],\n",
        "        'Error Rate (%)': metrics_df['error_rate'].iloc[-1],\n",
        "        'Model Accuracy': metrics_df['model_accuracy'].iloc[-1]\n",
        "    },\n",
        "    'Last Hour Averages': {\n",
        "        'CPU Usage (%)': last_hour_data['cpu_usage'].mean(),\n",
        "        'Memory Usage (%)': last_hour_data['memory_usage'].mean(),\n",
        "        'Response Time (ms)': last_hour_data['response_time_ms'].mean(),\n",
        "        'Error Rate (%)': last_hour_data['error_rate'].mean(),\n",
        "        'Requests/Second': last_hour_data['requests_per_second'].mean()\n",
        "    },\n",
        "    'Last 24h Summary': {\n",
        "        'Total Requests': int(last_24h_data['requests_per_second'].sum() * 5 / 60),  # Convert to total\n",
        "        'Avg Response Time (ms)': last_24h_data['response_time_ms'].mean(),\n",
        "        'Max Response Time (ms)': last_24h_data['response_time_ms'].max(),\n",
        "        'Avg Error Rate (%)': last_24h_data['error_rate'].mean(),\n",
        "        'Uptime (%)': 100 - (last_24h_data['error_rate'] > 5).mean() * 100\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üìä SYSTEM PERFORMANCE DASHBOARD\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for category, metrics in kpis.items():\n",
        "    print(f\"\\n{category}:\")\n",
        "    for metric, value in metrics.items():\n",
        "        if isinstance(value, float):\n",
        "            print(f\"  {metric}: {value:.2f}\")\n",
        "        else:\n",
        "            print(f\"  {metric}: {value:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Performance Tracking {#model-performance}\n",
        "\n",
        "Monitor model performance over time and detect drift."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate model performance data over time\n",
        "def generate_model_performance_data(days=30):\n",
        "    \"\"\"Generate synthetic model performance data\"\"\"\n",
        "    \n",
        "    dates = pd.date_range(start=datetime.now() - timedelta(days=days), \n",
        "                         end=datetime.now(), freq='H')\n",
        "    \n",
        "    performance_data = []\n",
        "    \n",
        "    # Simulate gradual model drift\n",
        "    base_accuracy = 0.95\n",
        "    drift_rate = 0.0001  # Small drift per hour\n",
        "    \n",
        "    for i, date in enumerate(dates):\n",
        "        # Add drift and noise\n",
        "        accuracy = base_accuracy - (i * drift_rate) + np.random.normal(0, 0.01)\n",
        "        accuracy = max(0.8, min(1.0, accuracy))  # Clamp between 0.8 and 1.0\n",
        "        \n",
        "        # Other metrics\n",
        "        precision = accuracy + np.random.normal(0, 0.005)\n",
        "        recall = accuracy + np.random.normal(0, 0.005)\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        \n",
        "        # Prediction volume\n",
        "        hour = date.hour\n",
        "        daily_pattern = 1.0 + 0.5 * np.sin((hour - 6) * np.pi / 12) if 6 <= hour <= 18 else 0.3\n",
        "        predictions = max(0, int(1000 * daily_pattern + np.random.normal(0, 100)))\n",
        "        \n",
        "        performance_data.append({\n",
        "            'timestamp': date,\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1_score,\n",
        "            'predictions_count': predictions,\n",
        "            'false_positives': int(predictions * (1 - precision) * recall),\n",
        "            'false_negatives': int(predictions * precision * (1 - recall))\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(performance_data)\n",
        "\n",
        "# Generate model performance data\n",
        "model_perf_df = generate_model_performance_data(days=30)\n",
        "print(f\"Generated {len(model_perf_df)} model performance records\")\n",
        "print(\"\\nSample data:\")\n",
        "print(model_perf_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model performance visualization\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    subplot_titles=('Model Accuracy Over Time', 'Precision vs Recall',\n",
        "                   'Prediction Volume', 'False Positives/Negatives'),\n",
        "    specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "           [{\"secondary_y\": True}, {\"secondary_y\": False}]]\n",
        ")\n",
        "\n",
        "# Accuracy over time with trend line\n",
        "fig.add_trace(go.Scatter(x=model_perf_df['timestamp'], y=model_perf_df['accuracy'],\n",
        "                        mode='lines', name='Accuracy', line=dict(color='blue')),\n",
        "             row=1, col=1)\n",
        "\n",
        "# Add trend line\n",
        "z = np.polyfit(range(len(model_perf_df)), model_perf_df['accuracy'], 1)\n",
        "trend_line = np.poly1d(z)(range(len(model_perf_df)))\n",
        "fig.add_trace(go.Scatter(x=model_perf_df['timestamp'], y=trend_line,\n",
        "                        mode='lines', name='Trend', line=dict(color='red', dash='dash')),\n",
        "             row=1, col=1)\n",
        "\n",
        "# Precision vs Recall scatter\n",
        "fig.add_trace(go.Scatter(x=model_perf_df['recall'], y=model_perf_df['precision'],\n",
        "                        mode='markers', name='Precision vs Recall',\n",
        "                        marker=dict(color=model_perf_df['f1_score'], colorscale='Viridis',\n",
        "                                  colorbar=dict(title=\"F1 Score\", x=0.48))),\n",
        "             row=1, col=2)\n",
        "\n",
        "# Prediction volume\n",
        "fig.add_trace(go.Scatter(x=model_perf_df['timestamp'], y=model_perf_df['predictions_count'],\n",
        "                        mode='lines', name='Predictions', line=dict(color='green')),\n",
        "             row=2, col=1)\n",
        "\n",
        "# False positives and negatives\n",
        "fig.add_trace(go.Scatter(x=model_perf_df['timestamp'], y=model_perf_df['false_positives'],\n",
        "                        mode='lines', name='False Positives', line=dict(color='orange')),\n",
        "             row=2, col=2)\n",
        "fig.add_trace(go.Scatter(x=model_perf_df['timestamp'], y=model_perf_df['false_negatives'],\n",
        "                        mode='lines', name='False Negatives', line=dict(color='red')),\n",
        "             row=2, col=2)\n",
        "\n",
        "fig.update_layout(height=800, title_text=\"Model Performance Analysis\", showlegend=True)\n",
        "fig.show()\n",
        "\n",
        "# Calculate drift statistics\n",
        "initial_accuracy = model_perf_df['accuracy'].iloc[:24].mean()  # First day\n",
        "recent_accuracy = model_perf_df['accuracy'].iloc[-24:].mean()  # Last day\n",
        "drift_magnitude = abs(recent_accuracy - initial_accuracy)\n",
        "\n",
        "print(f\"\\nüìà Model Drift Analysis:\")\n",
        "print(f\"Initial Accuracy (Day 1): {initial_accuracy:.4f}\")\n",
        "print(f\"Recent Accuracy (Last Day): {recent_accuracy:.4f}\")\n",
        "print(f\"Drift Magnitude: {drift_magnitude:.4f}\")\n",
        "print(f\"Drift Rate: {(recent_accuracy - initial_accuracy)/initial_accuracy*100:.2f}%\")\n",
        "\n",
        "if drift_magnitude > 0.01:\n",
        "    print(\"‚ö†Ô∏è  ALERT: Significant model drift detected!\")\n",
        "else:\n",
        "    print(\"‚úÖ Model performance is stable\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Quality Monitoring {#data-quality}\n",
        "\n",
        "Monitor data quality and feature distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate data quality metrics\n",
        "def generate_data_quality_metrics(days=7):\n",
        "    \"\"\"Generate synthetic data quality metrics\"\"\"\n",
        "    \n",
        "    dates = pd.date_range(start=datetime.now() - timedelta(days=days), \n",
        "                         end=datetime.now(), freq='H')\n",
        "    \n",
        "    quality_data = []\n",
        "    \n",
        "    for date in dates:\n",
        "        # Data completeness (percentage of non-null values)\n",
        "        completeness = max(85, min(100, 98 + np.random.normal(0, 2)))\n",
        "        \n",
        "        # Data freshness (delay in minutes)\n",
        "        freshness_delay = max(0, np.random.exponential(5))  # Exponential distribution\n",
        "        \n",
        "        # Feature distribution drift (KL divergence simulation)\n",
        "        feature_drift = abs(np.random.normal(0, 0.1))\n",
        "        \n",
        "        # Anomaly detection (percentage of anomalous records)\n",
        "        anomaly_rate = max(0, min(10, np.random.exponential(0.5)))\n",
        "        \n",
        "        # Schema violations\n",
        "        schema_violations = max(0, int(np.random.poisson(0.1)))\n",
        "        \n",
        "        # Record count\n",
        "        hour = date.hour\n",
        "        daily_pattern = 1.0 + 0.5 * np.sin((hour - 6) * np.pi / 12) if 6 <= hour <= 18 else 0.3\n",
        "        record_count = max(0, int(5000 * daily_pattern + np.random.normal(0, 500)))\n",
        "        \n",
        "        quality_data.append({\n",
        "            'timestamp': date,\n",
        "            'data_completeness': completeness,\n",
        "            'freshness_delay_minutes': freshness_delay,\n",
        "            'feature_drift_score': feature_drift,\n",
        "            'anomaly_rate': anomaly_rate,\n",
        "            'schema_violations': schema_violations,\n",
        "            'record_count': record_count\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(quality_data)\n",
        "\n",
        "# Generate data quality metrics\n",
        "data_quality_df = generate_data_quality_metrics(days=7)\n",
        "print(f\"Generated {len(data_quality_df)} data quality records\")\n",
        "print(\"\\nSample data:\")\n",
        "print(data_quality_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data quality visualization\n",
        "fig = make_subplots(\n",
        "    rows=3, cols=2,\n",
        "    subplot_titles=('Data Completeness (%)', 'Data Freshness (minutes)',\n",
        "                   'Feature Drift Score', 'Anomaly Rate (%)',\n",
        "                   'Schema Violations', 'Record Count'),\n",
        "    vertical_spacing=0.08\n",
        ")\n",
        "\n",
        "# Data Completeness\n",
        "fig.add_trace(go.Scatter(x=data_quality_df['timestamp'], y=data_quality_df['data_completeness'],\n",
        "                        mode='lines+markers', name='Completeness',\n",
        "                        line=dict(color='green')),\n",
        "             row=1, col=1)\n",
        "fig.add_hline(y=95, line_dash=\"dash\", line_color=\"red\", \n",
        "              annotation_text=\"Threshold\", row=1, col=1)\n",
        "\n",
        "# Data Freshness\n",
        "fig.add_trace(go.Scatter(x=data_quality_df['timestamp'], y=data_quality_df['freshness_delay_minutes'],\n",
        "                        mode='lines+markers', name='Freshness Delay',\n",
        "                        line=dict(color='blue')),\n",
        "             row=1, col=2)\n",
        "fig.add_hline(y=10, line_dash=\"dash\", line_color=\"red\", \n",
        "              annotation_text=\"SLA\", row=1, col=2)\n",
        "\n",
        "# Feature Drift\n",
        "fig.add_trace(go.Scatter(x=data_quality_df['timestamp'], y=data_quality_df['feature_drift_score'],\n",
        "                        mode='lines+markers', name='Feature Drift',\n",
        "                        line=dict(color='orange')),\n",
        "             row=2, col=1)\n",
        "fig.add_hline(y=0.2, line_dash=\"dash\", line_color=\"red\", \n",
        "              annotation_text=\"Alert Threshold\", row=2, col=1)\n",
        "\n",
        "# Anomaly Rate\n",
        "fig.add_trace(go.Scatter(x=data_quality_df['timestamp'], y=data_quality_df['anomaly_rate'],\n",
        "                        mode='lines+markers', name='Anomaly Rate',\n",
        "                        line=dict(color='red')),\n",
        "             row=2, col=2)\n",
        "\n",
        "# Schema Violations\n",
        "fig.add_trace(go.Bar(x=data_quality_df['timestamp'], y=data_quality_df['schema_violations'],\n",
        "                    name='Schema Violations', marker_color='purple'),\n",
        "             row=3, col=1)\n",
        "\n",
        "# Record Count\n",
        "fig.add_trace(go.Scatter(x=data_quality_df['timestamp'], y=data_quality_df['record_count'],\n",
        "                        mode='lines', name='Record Count',\n",
        "                        line=dict(color='darkgreen')),\n",
        "             row=3, col=2)\n",
        "\n",
        "fig.update_layout(height=1000, title_text=\"Data Quality Monitoring Dashboard\", showlegend=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data quality summary and alerts\n",
        "current_metrics = data_quality_df.iloc[-1]\n",
        "recent_24h = data_quality_df.iloc[-24:]\n",
        "\n",
        "# Define thresholds\n",
        "thresholds = {\n",
        "    'data_completeness': {'min': 95, 'direction': 'above'},\n",
        "    'freshness_delay_minutes': {'max': 10, 'direction': 'below'},\n",
        "    'feature_drift_score': {'max': 0.2, 'direction': 'below'},\n",
        "    'anomaly_rate': {'max': 3, 'direction': 'below'},\n",
        "    'schema_violations': {'max': 0, 'direction': 'below'}\n",
        "}\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üîç DATA QUALITY MONITORING REPORT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nüìä Current Status (as of {current_metrics['timestamp']})\")\n",
        "alerts = []\n",
        "\n",
        "for metric, threshold in thresholds.items():\n",
        "    current_value = current_metrics[metric]\n",
        "    \n",
        "    # Check if metric violates threshold\n",
        "    if threshold['direction'] == 'above' and 'min' in threshold:\n",
        "        violation = current_value < threshold['min']\n",
        "        status = \"‚ö†Ô∏è \" if violation else \"‚úÖ\"\n",
        "        threshold_text = f\"(threshold: >{threshold['min']})\"\n",
        "    elif threshold['direction'] == 'below' and 'max' in threshold:\n",
        "        violation = current_value > threshold['max']\n",
        "        status = \"‚ö†Ô∏è \" if violation else \"‚úÖ\"\n",
        "        threshold_text = f\"(threshold: <{threshold['max']})\"\n",
        "    else:\n",
        "        violation = False\n",
        "        status = \"‚úÖ\"\n",
        "        threshold_text = \"\"\n",
        "    \n",
        "    if violation:\n",
        "        alerts.append(f\"{metric}: {current_value:.2f} {threshold_text}\")\n",
        "    \n",
        "    print(f\"  {status} {metric.replace('_', ' ').title()}: {current_value:.2f} {threshold_text}\")\n",
        "\n",
        "print(f\"\\nüìà 24-Hour Trends:\")\n",
        "print(f\"  ‚Ä¢ Average Completeness: {recent_24h['data_completeness'].mean():.2f}%\")\n",
        "print(f\"  ‚Ä¢ Average Freshness Delay: {recent_24h['freshness_delay_minutes'].mean():.2f} minutes\")\n",
        "print(f\"  ‚Ä¢ Max Feature Drift: {recent_24h['feature_drift_score'].max():.3f}\")\n",
        "print(f\"  ‚Ä¢ Total Records Processed: {recent_24h['record_count'].sum():,}\")\n",
        "print(f\"  ‚Ä¢ Total Schema Violations: {recent_24h['schema_violations'].sum()}\")\n",
        "\n",
        "if alerts:\n",
        "    print(f\"\\nüö® ACTIVE ALERTS ({len(alerts)}):\")\n",
        "    for i, alert in enumerate(alerts, 1):\n",
        "        print(f\"  {i}. {alert}\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ No active data quality alerts\")\n",
        "\n",
        "print(f\"\\nüí° Recommendations:\")\n",
        "if recent_24h['data_completeness'].mean() < 98:\n",
        "    print(f\"  ‚Ä¢ Investigate data ingestion pipeline for completeness issues\")\n",
        "if recent_24h['freshness_delay_minutes'].mean() > 5:\n",
        "    print(f\"  ‚Ä¢ Optimize data pipeline for faster processing\")\n",
        "if recent_24h['feature_drift_score'].max() > 0.15:\n",
        "    print(f\"  ‚Ä¢ Monitor feature distributions for potential model retraining\")\n",
        "if recent_24h['anomaly_rate'].mean() > 2:\n",
        "    print(f\"  ‚Ä¢ Review anomaly detection rules and investigate root causes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Real-time Dashboard Simulation {#dashboard}\n",
        "\n",
        "Create a simulated real-time monitoring dashboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive monitoring dashboard\n",
        "def create_monitoring_dashboard():\n",
        "    \"\"\"Create a comprehensive monitoring dashboard\"\"\"\n",
        "    \n",
        "    # Get latest data points\n",
        "    latest_system = metrics_df.iloc[-1]\n",
        "    latest_model = model_perf_df.iloc[-1]\n",
        "    latest_quality = data_quality_df.iloc[-1]\n",
        "    \n",
        "    # Create dashboard with subplots\n",
        "    fig = make_subplots(\n",
        "        rows=4, cols=4,\n",
        "        subplot_titles=(\n",
        "            'CPU Usage', 'Memory Usage', 'Response Time', 'Error Rate',\n",
        "            'Model Accuracy', 'Predictions/Min', 'Data Completeness', 'Feature Drift',\n",
        "            'Request Volume (24h)', 'Model Performance Trend', 'Data Quality Score', 'System Health',\n",
        "            'Alert Summary', 'Performance Distribution', 'Resource Utilization', 'Uptime Status'\n",
        "        ),\n",
        "        specs=[\n",
        "            [{\"type\": \"indicator\"}, {\"type\": \"indicator\"}, {\"type\": \"indicator\"}, {\"type\": \"indicator\"}],\n",
        "            [{\"type\": \"indicator\"}, {\"type\": \"indicator\"}, {\"type\": \"indicator\"}, {\"type\": \"indicator\"}],\n",
        "            [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}, {\"type\": \"scatter\"}, {\"type\": \"scatter\"}],\n",
        "            [{\"type\": \"table\"}, {\"type\": \"histogram\"}, {\"type\": \"pie\"}, {\"type\": \"indicator\"}]\n",
        "        ],\n",
        "        vertical_spacing=0.08\n",
        "    )\n",
        "    \n",
        "    # Row 1: Key Indicators\n",
        "    # CPU Usage\n",
        "    fig.add_trace(go.Indicator(\n",
        "        mode=\"gauge+number+delta\",\n",
        "        value=latest_system['cpu_usage'],\n",
        "        domain={'x': [0, 1], 'y': [0, 1]},\n",
        "        title={'text': \"CPU %\"},\n",
        "        delta={'reference': 50},\n",
        "        gauge={'axis': {'range': [None, 100]},\n",
        "               'bar': {'color': \"darkblue\"},\n",
        "               'steps': [{'range': [0, 50], 'color': \"lightgray\"},\n",
        "                        {'range': [50, 80], 'color': \"yellow\"}],\n",
        "               'threshold': {'line': {'color': \"red\", 'width': 4},\n",
        "                           'thickness': 0.75, 'value': 90}}\n",
        "    ), row=1, col=1)\n",
        "    \n",
        "    # Memory Usage\n",
        "    fig.add_trace(go.Indicator(\n",
        "        mode=\"gauge+number\",\n",
        "        value=latest_system['memory_usage'],\n",
        "        title={'text': \"Memory %\"},\n",
        "        gauge={'axis': {'range': [None, 100]},\n",
        "               'bar': {'color': \"darkgreen\"},\n",
        "               'threshold': {'line': {'color': \"red\", 'width': 4},\n",
        "                           'thickness': 0.75, 'value': 85}}\n",
        "    ), row=1, col=2)\n",
        "    \n",
        "    # Response Time\n",
        "    fig.add_trace(go.Indicator(\n",
        "        mode=\"gauge+number\",\n",
        "        value=latest_system['response_time_ms'],\n",
        "        title={'text': \"Response Time (ms)\"},\n",
        "        gauge={'axis': {'range': [0, 500]},\n",
        "               'bar': {'color': \"orange\"},\n",
        "               'threshold': {'line': {'color': \"red\", 'width': 4},\n",
        "                           'thickness': 0.75, 'value': 300}}\n",
        "    ), row=1, col=3)\n",
        "    \n",
        "    # Error Rate\n",
        "    fig.add_trace(go.Indicator(\n",
        "        mode=\"number+delta\",\n",
        "        value=latest_system['error_rate'],\n",
        "        title={'text': \"Error Rate %\"},\n",
        "        delta={'reference': 1, 'increasing': {'color': \"red\"}}\n",
        "    ), row=1, col=4)\n",
        "    \n",
        "    # Row 2: Model Indicators\n",
        "    # Model Accuracy\n",
        "    fig.add_trace(go.Indicator(\n",
        "        mode=\"gauge+number\",\n",
        "        value=latest_model['accuracy'],\n",
        "        title={'text': \"Model Accuracy\"},\n",
        "        gauge={'axis': {'range': [0.8, 1.0]},\n",
        "               'bar': {'color': \"purple\"},\n",
        "               'threshold': {'line': {'color': \"red\", 'width': 4},\n",
        "                           'thickness': 0.75, 'value': 0.9}}\n",
        "    ), row=2, col=1)\n",
        "    \n",
        "    # Predictions per minute\n",
        "    fig.add_trace(go.Indicator(\n",
        "        mode=\"number+delta\",\n",
        "        value=latest_system['predictions_per_minute'],\n",
        "        title={'text': \"Predictions/Min\"},\n",
        "        delta={'reference': 50}\n",
        "    ), row=2, col=2)\n",
        "    \n",
        "    # Data Completeness\n",
        "    fig.add_trace(go.Indicator(\n",
        "        mode=\"gauge+number\",\n",
        "        value=latest_quality['data_completeness'],\n",
        "        title={'text': \"Data Completeness %\"},\n",
        "        gauge={'axis': {'range': [90, 100]},\n",
        "               'bar': {'color': \"green\"},\n",
        "               'threshold': {'line': {'color': \"red\", 'width': 4},\n",
        "                           'thickness': 0.75, 'value': 95}}\n",
        "    ), row=2, col=3)\n",
        "    \n",
        "    # Feature Drift\n",
        "    fig.add_trace(go.Indicator(\n",
        "        mode=\"number+delta\",\n",
        "        value=latest_quality['feature_drift_score'],\n",
        "        title={'text': \"Feature Drift\"},\n",
        "        delta={'reference': 0.1, 'increasing': {'color': \"red\"}}\n",
        "    ), row=2, col=4)\n",
        "    \n",
        "    # Row 3: Time Series\n",
        "    # Request volume (last 24h)\n",
        "    last_24h_metrics = metrics_df.iloc[-288:]  # Last 24 hours (5 min intervals)\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=last_24h_metrics['timestamp'], \n",
        "        y=last_24h_metrics['requests_per_second'],\n",
        "        mode='lines', name='Requests/sec',\n",
        "        line=dict(color='blue')\n",
        "    ), row=3, col=1)\n",
        "    \n",
        "    # Model performance trend\n",
        "    recent_model = model_perf_df.iloc[-48:]  # Last 48 hours\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=recent_model['timestamp'], \n",
        "        y=recent_model['accuracy'],\n",
        "        mode='lines', name='Accuracy',\n",
        "        line=dict(color='purple')\n",
        "    ), row=3, col=2)\n",
        "    \n",
        "    # Data quality score (composite)\n",
        "    data_quality_df['quality_score'] = (\n",
        "        data_quality_df['data_completeness'] / 100 * 0.4 +\n",
        "        np.maximum(0, 1 - data_quality_df['freshness_delay_minutes'] / 60) * 0.3 +\n",
        "        np.maximum(0, 1 - data_quality_df['feature_drift_score'] / 0.5) * 0.3\n",
        "    ) * 100\n",
        "    \n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=data_quality_df['timestamp'], \n",
        "        y=data_quality_df['quality_score'],\n",
        "        mode='lines', name='Quality Score',\n",
        "        line=dict(color='green')\n",
        "    ), row=3, col=3)\n",
        "    \n",
        "    # System health (composite)\n",
        "    metrics_df['health_score'] = (\n",
        "        np.maximum(0, 1 - metrics_df['cpu_usage'] / 100) * 0.25 +\n",
        "        np.maximum(0, 1 - metrics_df['memory_usage'] / 100) * 0.25 +\n",
        "        np.maximum(0, 1 - metrics_df['response_time_ms'] / 1000) * 0.25 +\n",
        "        np.maximum(0, 1 - metrics_df['error_rate'] / 10) * 0.25\n",
        "    ) * 100\n",
        "    \n",
        "    recent_health = metrics_df.iloc[-288:]  # Last 24 hours\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=recent_health['timestamp'], \n",
        "        y=recent_health['health_score'],\n",
        "        mode='lines', name='Health Score',\n",
        "        line=dict(color='red')\n",
        "    ), row=3, col=4)\n",
        "    \n",
        "    fig.update_layout(\n",
        "        height=1200, \n",
        "        title_text=\"üñ•Ô∏è ML Pipeline Platform - Real-time Monitoring Dashboard\",\n",
        "        showlegend=False\n",
        "    )\n",
        "    \n",
        "    return fig\n",
        "\n",
        "# Create and display dashboard\n",
        "dashboard = create_monitoring_dashboard()\n",
        "dashboard.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Alert System Analysis {#alerts}\n",
        "\n",
        "Analyze and simulate alert conditions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define alert rules and check conditions\n",
        "def check_alerts(metrics_df, model_perf_df, data_quality_df):\n",
        "    \"\"\"Check all alert conditions and generate alerts\"\"\"\n",
        "    \n",
        "    alerts = []\n",
        "    current_time = datetime.now()\n",
        "    \n",
        "    # Get latest values\n",
        "    latest_system = metrics_df.iloc[-1]\n",
        "    latest_model = model_perf_df.iloc[-1]\n",
        "    latest_quality = data_quality_df.iloc[-1]\n",
        "    \n",
        "    # Recent data for trend analysis\n",
        "    recent_system = metrics_df.iloc[-12:]  # Last hour (5-min intervals)\n",
        "    recent_model = model_perf_df.iloc[-24:]  # Last 24 hours\n",
        "    recent_quality = data_quality_df.iloc[-6:]  # Last 6 hours\n",
        "    \n",
        "    # System Performance Alerts\n",
        "    if latest_system['cpu_usage'] > 90:\n",
        "        alerts.append({\n",
        "            'severity': 'CRITICAL',\n",
        "            'category': 'System',\n",
        "            'message': f\"High CPU usage: {latest_system['cpu_usage']:.1f}%\",\n",
        "            'value': latest_system['cpu_usage'],\n",
        "            'threshold': 90,\n",
        "            'timestamp': current_time\n",
        "        })\n",
        "    \n",
        "    if latest_system['memory_usage'] > 85:\n",
        "        alerts.append({\n",
        "            'severity': 'WARNING',\n",
        "            'category': 'System',\n",
        "            'message': f\"High memory usage: {latest_system['memory_usage']:.1f}%\",\n",
        "            'value': latest_system['memory_usage'],\n",
        "            'threshold': 85,\n",
        "            'timestamp': current_time\n",
        "        })\n",
        "    \n",
        "    if latest_system['response_time_ms'] > 300:\n",
        "        alerts.append({\n",
        "            'severity': 'WARNING',\n",
        "            'category': 'Performance',\n",
        "            'message': f\"High response time: {latest_system['response_time_ms']:.1f}ms\",\n",
        "            'value': latest_system['response_time_ms'],\n",
        "            'threshold': 300,\n",
        "            'timestamp': current_time\n",
        "        })\n",
        "    \n",
        "    if latest_system['error_rate'] > 5:\n",
        "        alerts.append({\n",
        "            'severity': 'CRITICAL',\n",
        "            'category': 'Reliability',\n",
        "            'message': f\"High error rate: {latest_system['error_rate']:.2f}%\",\n",
        "            'value': latest_system['error_rate'],\n",
        "            'threshold': 5,\n",
        "            'timestamp': current_time\n",
        "        })\n",
        "    \n",
        "    # Model Performance Alerts\n",
        "    if latest_model['accuracy'] < 0.90:\n",
        "        alerts.append({\n",
        "            'severity': 'WARNING',\n",
        "            'category': 'Model',\n",
        "            'message': f\"Low model accuracy: {latest_model['accuracy']:.3f}\",\n",
        "            'value': latest_model['accuracy'],\n",
        "            'threshold': 0.90,\n",
        "            'timestamp': current_time\n",
        "        })\n",
        "    \n",
        "    # Model drift detection\n",
        "    if len(recent_model) >= 24:\n",
        "        accuracy_trend = recent_model['accuracy'].iloc[-1] - recent_model['accuracy'].iloc[0]\n",
        "        if accuracy_trend < -0.02:  # 2% drop in 24 hours\n",
        "            alerts.append({\n",
        "                'severity': 'WARNING',\n",
        "                'category': 'Model',\n",
        "                'message': f\"Model accuracy declining: {accuracy_trend:.3f} in 24h\",\n",
        "                'value': accuracy_trend,\n",
        "                'threshold': -0.02,\n",
        "                'timestamp': current_time\n",
        "            })\n",
        "    \n",
        "    # Data Quality Alerts\n",
        "    if latest_quality['data_completeness'] < 95:\n",
        "        alerts.append({\n",
        "            'severity': 'WARNING',\n",
        "            'category': 'Data Quality',\n",
        "            'message': f\"Low data completeness: {latest_quality['data_completeness']:.1f}%\",\n",
        "            'value': latest_quality['data_completeness'],\n",
        "            'threshold': 95,\n",
        "            'timestamp': current_time\n",
        "        })\n",
        "    \n",
        "    if latest_quality['freshness_delay_minutes'] > 10:\n",
        "        alerts.append({\n",
        "            'severity': 'WARNING',\n",
        "            'category': 'Data Quality',\n",
        "            'message': f\"Data freshness delay: {latest_quality['freshness_delay_minutes']:.1f} minutes\",\n",
        "            'value': latest_quality['freshness_delay_minutes'],\n",
        "            'threshold': 10,\n",
        "            'timestamp': current_time\n",
        "        })\n",
        "    \n",
        "    if latest_quality['feature_drift_score'] > 0.2:\n",
        "        alerts.append({\n",
        "            'severity': 'CRITICAL',\n",
        "            'category': 'Data Quality',\n",
        "            'message': f\"High feature drift: {latest_quality['feature_drift_score']:.3f}\",\n",
        "            'value': latest_quality['feature_drift_score'],\n",
        "            'threshold': 0.2,\n",
        "            'timestamp': current_time\n",
        "        })\n",
        "    \n",
        "    # Volume anomaly detection\n",
        "    if len(recent_system) >= 12:\n",
        "        avg_requests = recent_system['requests_per_second'].mean()\n",
        "        current_requests = latest_system['requests_per_second']\n",
        "        if current_requests < avg_requests * 0.3:  # 70% drop\n",
        "            alerts.append({\n",
        "                'severity': 'CRITICAL',\n",
        "                'category': 'Traffic',\n",
        "                'message': f\"Low traffic volume: {current_requests:.1f} req/s (avg: {avg_requests:.1f})\",\n",
        "                'value': current_requests,\n",
        "                'threshold': avg_requests * 0.3,\n",
        "                'timestamp': current_time\n",
        "            })\n",
        "        elif current_requests > avg_requests * 2:  # 100% increase\n",
        "            alerts.append({\n",
        "                'severity': 'WARNING',\n",
        "                'category': 'Traffic',\n",
        "                'message': f\"High traffic spike: {current_requests:.1f} req/s (avg: {avg_requests:.1f})\",\n",
        "                'value': current_requests,\n",
        "                'threshold': avg_requests * 2,\n",
        "                'timestamp': current_time\n",
        "            })\n",
        "    \n",
        "    return alerts\n",
        "\n",
        "# Check for alerts\n",
        "current_alerts = check_alerts(metrics_df, model_perf_df, data_quality_df)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üö® ALERT SYSTEM STATUS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if current_alerts:\n",
        "    # Sort alerts by severity\n",
        "    severity_order = {'CRITICAL': 0, 'WARNING': 1, 'INFO': 2}\n",
        "    current_alerts.sort(key=lambda x: severity_order.get(x['severity'], 3))\n",
        "    \n",
        "    print(f\"\\nüî• ACTIVE ALERTS ({len(current_alerts)}):\")\n",
        "    \n",
        "    for i, alert in enumerate(current_alerts, 1):\n",
        "        severity_emoji = \"üî¥\" if alert['severity'] == 'CRITICAL' else \"üü°\" if alert['severity'] == 'WARNING' else \"üîµ\"\n",
        "        print(f\"\\n{i}. {severity_emoji} {alert['severity']} - {alert['category']}\")\n",
        "        print(f\"   {alert['message']}\")\n",
        "        print(f\"   Time: {alert['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    \n",
        "    # Alert summary by category and severity\n",
        "    alert_df = pd.DataFrame(current_alerts)\n",
        "    \n",
        "    print(f\"\\nüìä Alert Summary:\")\n",
        "    print(alert_df.groupby(['severity', 'category']).size().to_string())\n",
        "    \nelse:\n",
        "    print(f\"\\n‚úÖ No active alerts - System operating normally\")\n",
        "\n",
        "print(f\"\\nüìã Alert Configuration:\")\n",
        "print(f\"   ‚Ä¢ CPU Usage: >90% (Critical), >80% (Warning)\")\n",
        "print(f\"   ‚Ä¢ Memory Usage: >85% (Warning)\")\n",
        "print(f\"   ‚Ä¢ Response Time: >300ms (Warning), >500ms (Critical)\")\n",
        "print(f\"   ‚Ä¢ Error Rate: >5% (Critical), >2% (Warning)\")\n",
        "print(f\"   ‚Ä¢ Model Accuracy: <90% (Warning), <85% (Critical)\")\n",
        "print(f\"   ‚Ä¢ Data Completeness: <95% (Warning), <90% (Critical)\")\n",
        "print(f\"   ‚Ä¢ Feature Drift: >0.2 (Critical), >0.1 (Warning)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Performance Optimization {#optimization}\n",
        "\n",
        "Analyze performance bottlenecks and optimization opportunities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance optimization analysis\n",
        "def analyze_performance_bottlenecks(metrics_df, model_perf_df):\n",
        "    \"\"\"Identify performance bottlenecks and optimization opportunities\"\"\"\n",
        "    \n",
        "    optimization_recommendations = []\n",
        "    \n",
        "    # Analyze system metrics\n",
        "    avg_cpu = metrics_df['cpu_usage'].mean()\n",
        "    avg_memory = metrics_df['memory_usage'].mean()\n",
        "    avg_response_time = metrics_df['response_time_ms'].mean()\n",
        "    avg_requests = metrics_df['requests_per_second'].mean()\n",
        "    \n",
        "    # CPU Analysis\n",
        "    if avg_cpu > 70:\n",
        "        optimization_recommendations.append({\n",
        "            'category': 'CPU',\n",
        "            'priority': 'High',\n",
        "            'issue': f'High average CPU usage: {avg_cpu:.1f}%',\n",
        "            'recommendation': 'Consider horizontal scaling or CPU optimization',\n",
        "            'impact': 'Performance degradation under load'\n",
        "        })\n",
        "    \n",
        "    # Memory Analysis\n",
        "    if avg_memory > 60:\n",
        "        optimization_recommendations.append({\n",
        "            'category': 'Memory',\n",
        "            'priority': 'Medium',\n",
        "            'issue': f'High average memory usage: {avg_memory:.1f}%',\n",
        "            'recommendation': 'Optimize memory usage or increase available memory',\n",
        "            'impact': 'Risk of out-of-memory errors'\n",
        "        })\n",
        "    \n",
        "    # Response Time Analysis\n",
        "    response_p95 = metrics_df['response_time_ms'].quantile(0.95)\n",
        "    if response_p95 > 200:\n",
        "        optimization_recommendations.append({\n",
        "            'category': 'Latency',\n",
        "            'priority': 'High',\n",
        "            'issue': f'High P95 response time: {response_p95:.1f}ms',\n",
        "            'recommendation': 'Optimize API endpoints and database queries',\n",
        "            'impact': 'Poor user experience'\n",
        "        })\n",
        "    \n",
        "    # Model Performance Analysis\n",
        "    recent_accuracy = model_perf_df['accuracy'].iloc[-168:].mean()  # Last week\n",
        "    initial_accuracy = model_perf_df['accuracy'].iloc[:168].mean()  # First week\n",
        "    \n",
        "    if recent_accuracy < initial_accuracy - 0.01:\n",
        "        optimization_recommendations.append({\n",
        "            'category': 'Model',\n",
        "            'priority': 'High',\n",
        "            'issue': f'Model accuracy decline: {recent_accuracy:.3f} vs {initial_accuracy:.3f}',\n",
        "            'recommendation': 'Retrain model with recent data',\n",
        "            'impact': 'Reduced prediction quality'\n",
        "        })\n",
        "    \n",
        "    # Throughput Analysis\n",
        "    max_throughput = metrics_df['requests_per_second'].max()\n",
        "    if avg_requests / max_throughput < 0.3:  # Low utilization\n",
        "        optimization_recommendations.append({\n",
        "            'category': 'Capacity',\n",
        "            'priority': 'Low',\n",
        "            'issue': f'Low resource utilization: {avg_requests/max_throughput*100:.1f}%',\n",
        "            'recommendation': 'Consider downsizing resources or handling more traffic',\n",
        "            'impact': 'Cost optimization opportunity'\n",
        "        })\n",
        "    \n",
        "    # Error Rate Analysis\n",
        "    error_trend = metrics_df['error_rate'].diff().mean()\n",
        "    if error_trend > 0.01:  # Increasing error rate\n",
        "        optimization_recommendations.append({\n",
        "            'category': 'Reliability',\n",
        "            'priority': 'High',\n",
        "            'issue': f'Increasing error rate trend: +{error_trend:.3f}%/hour',\n",
        "            'recommendation': 'Investigate root cause and implement error handling',\n",
        "            'impact': 'Service reliability concerns'\n",
        "        })\n",
        "    \n",
        "    return optimization_recommendations\n",
        "\n",
        "# Analyze performance\n",
        "recommendations = analyze_performance_bottlenecks(metrics_df, model_perf_df)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"‚ö° PERFORMANCE OPTIMIZATION ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if recommendations:\n",
        "    # Sort by priority\n",
        "    priority_order = {'High': 0, 'Medium': 1, 'Low': 2}\n",
        "    recommendations.sort(key=lambda x: priority_order.get(x['priority'], 3))\n",
        "    \n",
        "    print(f\"\\nüîç Identified {len(recommendations)} optimization opportunities:\")\n",
        "    \n",
        "    for i, rec in enumerate(recommendations, 1):\n",
        "        priority_emoji = \"üî¥\" if rec['priority'] == 'High' else \"üü°\" if rec['priority'] == 'Medium' else \"üü¢\"\n",
        "        print(f\"\\n{i}. {priority_emoji} {rec['priority']} Priority - {rec['category']}\")\n",
        "        print(f\"   Issue: {rec['issue']}\")\n",
        "        print(f\"   Recommendation: {rec['recommendation']}\")\n",
        "        print(f\"   Impact: {rec['impact']}\")\n",
        "    \n",
        "    # Summary by category\n",
        "    rec_df = pd.DataFrame(recommendations)\n",
        "    print(f\"\\nüìä Optimization Summary:\")\n",
        "    print(rec_df.groupby(['priority', 'category']).size().to_string())\n",
        "    \nelse:\n",
        "    print(f\"\\n‚úÖ No major performance issues identified\")\n",
        "    print(f\"   System is operating within optimal parameters\")\n",
        "\n",
        "# Performance metrics summary\n",
        "print(f\"\\nüìà Current Performance Metrics:\")\n",
        "print(f\"   ‚Ä¢ Average CPU Usage: {metrics_df['cpu_usage'].mean():.1f}%\")\n",
        "print(f\"   ‚Ä¢ Average Memory Usage: {metrics_df['memory_usage'].mean():.1f}%\")\n",
        "print(f\"   ‚Ä¢ Average Response Time: {metrics_df['response_time_ms'].mean():.1f}ms\")\n",
        "print(f\"   ‚Ä¢ P95 Response Time: {metrics_df['response_time_ms'].quantile(0.95):.1f}ms\")\n",
        "print(f\"   ‚Ä¢ P99 Response Time: {metrics_df['response_time_ms'].quantile(0.99):.1f}ms\")\n",
        "print(f\"   ‚Ä¢ Average Requests/Second: {metrics_df['requests_per_second'].mean():.1f}\")\n",
        "print(f\"   ‚Ä¢ Average Error Rate: {metrics_df['error_rate'].mean():.2f}%\")\n",
        "print(f\"   ‚Ä¢ Model Accuracy (Recent): {model_perf_df['accuracy'].iloc[-24:].mean():.3f}\")\n",
        "\n",
        "print(f\"\\nüéØ Performance Targets:\")\n",
        "print(f\"   ‚Ä¢ Response Time P95: <200ms\")\n",
        "print(f\"   ‚Ä¢ Response Time P99: <500ms\")\n",
        "print(f\"   ‚Ä¢ Error Rate: <1%\")\n",
        "print(f\"   ‚Ä¢ CPU Usage: <70%\")\n",
        "print(f\"   ‚Ä¢ Model Accuracy: >95%\")\n",
        "print(f\"   ‚Ä¢ Uptime: >99.9%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance trends and capacity planning\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "\n",
        "# Resource utilization trend\n",
        "axes[0, 0].plot(metrics_df.index, metrics_df['cpu_usage'], label='CPU %', alpha=0.7)\n",
        "axes[0, 0].plot(metrics_df.index, metrics_df['memory_usage'], label='Memory %', alpha=0.7)\n",
        "axes[0, 0].axhline(y=80, color='r', linestyle='--', alpha=0.5, label='Target Threshold')\n",
        "axes[0, 0].set_title('Resource Utilization Trend')\n",
        "axes[0, 0].set_ylabel('Usage %')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Response time distribution\n",
        "axes[0, 1].hist(metrics_df['response_time_ms'], bins=30, alpha=0.7, edgecolor='black')\n",
        "axes[0, 1].axvline(x=metrics_df['response_time_ms'].mean(), color='r', linestyle='-', label=f\"Mean: {metrics_df['response_time_ms'].mean():.0f}ms\")\n",
        "axes[0, 1].axvline(x=metrics_df['response_time_ms'].quantile(0.95), color='orange', linestyle='--', label=f\"P95: {metrics_df['response_time_ms'].quantile(0.95):.0f}ms\")\n",
        "axes[0, 1].set_title('Response Time Distribution')\n",
        "axes[0, 1].set_xlabel('Response Time (ms)')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Throughput vs Response Time correlation\n",
        "scatter = axes[1, 0].scatter(metrics_df['requests_per_second'], metrics_df['response_time_ms'], \n",
        "                           c=metrics_df['cpu_usage'], cmap='viridis', alpha=0.6)\n",
        "axes[1, 0].set_title('Throughput vs Response Time (colored by CPU)')\n",
        "axes[1, 0].set_xlabel('Requests per Second')\n",
        "axes[1, 0].set_ylabel('Response Time (ms)')\n",
        "plt.colorbar(scatter, ax=axes[1, 0], label='CPU Usage %')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Model performance stability\n",
        "daily_accuracy = model_perf_df.set_index('timestamp').resample('D')['accuracy'].mean()\n",
        "axes[1, 1].plot(daily_accuracy.index, daily_accuracy.values, marker='o', linewidth=2)\n",
        "axes[1, 1].axhline(y=0.95, color='r', linestyle='--', alpha=0.5, label='Target: 95%')\n",
        "axes[1, 1].set_title('Daily Model Accuracy Trend')\n",
        "axes[1, 1].set_ylabel('Accuracy')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Capacity planning recommendations\n",
        "max_observed_rps = metrics_df['requests_per_second'].max()\n",
        "avg_rps = metrics_df['requests_per_second'].mean()\n",
        "growth_capacity = (max_observed_rps - avg_rps) / avg_rps * 100\n",
        "\n",
        "print(f\"\\nüîÆ CAPACITY PLANNING INSIGHTS:\")\n",
        "print(f\"   ‚Ä¢ Current Average Load: {avg_rps:.1f} req/s\")\n",
        "print(f\"   ‚Ä¢ Peak Load Observed: {max_observed_rps:.1f} req/s\")\n",
        "print(f\"   ‚Ä¢ Growth Capacity: {growth_capacity:.1f}% above average\")\n",
        "print(f\"   ‚Ä¢ Recommended Scaling Threshold: {avg_rps * 1.5:.1f} req/s\")\n",
        "print(f\"   ‚Ä¢ Estimated Breaking Point: {max_observed_rps * 1.2:.1f} req/s\")\n",
        "\n",
        "if growth_capacity < 50:\n",
        "    print(f\"   ‚ö†Ô∏è  Consider adding capacity - low headroom for traffic spikes\")\n",
        "else:\n",
        "    print(f\"   ‚úÖ Adequate capacity for handling traffic variations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Conclusion\n",
        "\n",
        "This performance monitoring notebook has provided comprehensive insights into the ML Pipeline Platform's operational health:\n",
        "\n",
        "### Key Monitoring Areas:\n",
        "- **System Performance**: CPU, memory, response times, and throughput\n",
        "- **Model Performance**: Accuracy trends, drift detection, and prediction volume\n",
        "- **Data Quality**: Completeness, freshness, and feature drift monitoring\n",
        "- **Real-time Alerts**: Automated detection of performance anomalies\n",
        "\n",
        "### Monitoring Best Practices:\n",
        "1. **Proactive Alerting**: Set appropriate thresholds for early warning\n",
        "2. **Trend Analysis**: Monitor long-term patterns for capacity planning\n",
        "3. **Multi-dimensional Monitoring**: Combine system, model, and data metrics\n",
        "4. **Performance Optimization**: Regular analysis for bottleneck identification\n",
        "\n",
        "### Next Steps:\n",
        "1. **Implement Dashboard**: Deploy real-time monitoring dashboard\n",
        "2. **Alert Integration**: Connect alerts to incident management systems\n",
        "3. **Automated Responses**: Implement auto-scaling and self-healing\n",
        "4. **Historical Analysis**: Maintain long-term performance baselines\n",
        "\n",
        "This monitoring framework ensures the ML Pipeline Platform operates reliably at scale while maintaining optimal performance and data quality."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}